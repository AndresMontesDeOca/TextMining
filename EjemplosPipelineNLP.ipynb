{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresMontesDeOca/TextMining/blob/main/EjemplosPipelineNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2L4HXw1TeUW"
      },
      "source": [
        "# Ejemplos NLTK\n",
        "\n",
        "Codigo fuente de este notebook ha sido extraido de:\n",
        "\n",
        "* Deep Learning for Natural Language Processing\n",
        "* Medium Word2Vec Model\n",
        "* Kavita Ganesan - Gensim Word2Vec\n",
        "* Python Text Processing with NLTK 2.0 Cookbook\n",
        "\n",
        "Ejemplo de pipeline:\n",
        "* Segmentacion de sentencias\n",
        "* Tokenizacion de palabras\n",
        "* Pos tagger\n",
        "* Lematizacion\n",
        "* Stemmer\n",
        "* Bag of words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAuvaHjZUNNe"
      },
      "source": [
        "## Importar librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqnR8sM_UMfA",
        "outputId": "57154ef3-448c-4de4-efb9-aca363791cb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# NLTK con Google Colab\n",
        "import os\n",
        "import nltk\n",
        "# nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r88tr159Ui8M"
      },
      "outputs": [],
      "source": [
        "! pip install -U -q spacy\n",
        "! python -m spacy download es_core_news_lg\n",
        "! python -m spacy download es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ThOlR4bTVHM"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tIzFCg0TuTw"
      },
      "source": [
        "## Tokenizacion\n",
        "Es una unidad útil en el procesamiento del lenguaje posterior, puede ser una palabra, una frase, un párrafo. Generalmente se realiza una tokenización de acuerdo a las palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEZic-1sVJOT"
      },
      "outputs": [],
      "source": [
        "text=\"\"\"El ministro de Salud porteño, Dr. Fernán Quirós, aseguró que hubo acuerdo para limitar el transporte público, la nocturnidad y las reuniones sociales. Sin embargo, hay distintas posturas con los gobiernos nacional y bonaerense sobre qué temperamentos se deben adoptar\"\"\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARZnQWRDT9t-"
      },
      "outputs": [],
      "source": [
        "# Tokenizacion por sentencias con el idioma español\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text_sent = sent_tokenize(text, language='spanish')\n",
        "text_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0BzAUowVkbf"
      },
      "outputs": [],
      "source": [
        "# Otra forma de tokenizar sentencias teniendo en cuenta el idioma español\n",
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
        "spanish_tokens = spanish_tokenizer.tokenize(text)\n",
        "\n",
        "spanish_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofl7s7ymVH4l"
      },
      "outputs": [],
      "source": [
        "# Tokenizacion por palabras\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "pp.pprint(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9OBijHvXI8W"
      },
      "outputs": [],
      "source": [
        "# Tokenizacion con expreciones reguales\n",
        "# Pagina para probar las expreciones regulaes -> https://regexr.com/\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "reg_exp_tokens = regexp_tokenize(text, \"[\\w']+\")\n",
        "pp.pprint(reg_exp_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlZD0hrLXozr"
      },
      "outputs": [],
      "source": [
        "# Separación en palabras mediante expresiones regulares definiendo el espacio como separador\n",
        "reg_exp_tokens = regexp_tokenize(text, '\\s+', gaps=True)\n",
        "pp.pprint(reg_exp_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK1Re_rxZXqF"
      },
      "source": [
        "## Stop words\n",
        "Son listados de palabras que no agregan valor para el procesamiento de NLP. Eliminarlas no cambia el significado de la oración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N5NNp5sZaB_"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import pprint\n",
        "\n",
        "english_stops = set(stopwords.words('english'))\n",
        "pp.pprint(english_stops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dreH40WTZmvL"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "spanish_stops = set( stopwords.words('spanish'))\n",
        "spanish_stops.update(string.punctuation) # Tambien le agrego los simbolos y signos de puntuacion -> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "pp.pprint(len(spanish_stops))\n",
        "\n",
        "words_eliminar = ['buenas','hola','tardes']\n",
        "spanish_stops.update(words_eliminar) # Tambien le agrego los simbolos y signos de puntuacion -> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "pp.pprint(len(spanish_stops))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXrzC_KbZ3z-"
      },
      "outputs": [],
      "source": [
        "# Filtrar los stopwords en el texto\n",
        "text_token = word_tokenize(text)\n",
        "pp.pprint(text_token)\n",
        "text_token_stopwords = [x.lower() for x in text_token if x.lower() not in spanish_stops]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJQ-4CKRAs95"
      },
      "outputs": [],
      "source": [
        "pp.pprint(text_token_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g34N2n7t4A-"
      },
      "source": [
        "## Stemmer\n",
        "Utilizado para obtener la palabra canónica. Este tipo de proceso se encarga de  truncar la palabra descartando el género y el tiempo\n",
        "\n",
        "Por ejemplo\n",
        "\n",
        "* corriendo -> corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1x2u4L2uCkA"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "palabras=['corro','correr', 'corriendo','corredor']\n",
        "for p in palabras:\n",
        "  print(stemmer.stem(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyXNgypVu9p5"
      },
      "source": [
        "## Lematizacion\n",
        "Se utiliza para obtener la palabra canónica, es decir el origen de la palabra. Igual que en el stemmer se eliminar el tiempo verbal y el género, pero se obtiene la palabra completa\n",
        "\n",
        "Por ejemplo\n",
        "* corriendo -> Correr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS8EYmB2vG2E"
      },
      "outputs": [],
      "source": [
        "# Lematizador con Spacy\n",
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "doc = nlp(\" \".join(palabras))\n",
        "\n",
        "for token in doc:\n",
        "  print(token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKDhvCmzLr_"
      },
      "source": [
        "## PoS tagging\n",
        "* Proceso de clasificación de las palabras de acuerdo a su función, por ejemplo sustantivo, adjetivo, verbo..\n",
        "\n",
        "* ¡Importante! Se debe utilizar antes de realizar procesos que modifican y descartan los tokens.\n",
        "\n",
        "* Existen varias clasificaciones ->  https://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\n",
        "\n",
        "* Por ejemplo si necesito conocer las cualidades de un objeto solamente tengo que capturar los adjetivos. También es útil en el análisis de sentimiento\n",
        "\n",
        "* Las palabras pueden ser morfológicamente idénticas pero dependiendo de la ubicación en la frase pueden tener diferente significado. Por ejemplo la palabra presente\n",
        "  * Recibí un hermoso **presente** de parte de mis amigos.\n",
        "  * Me **presente** a rendir la tesis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D1YmGOPzkLz"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\" \".join(palabras))\n",
        "\n",
        "for token in doc:\n",
        "  print(token, token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_dYSlIPU175"
      },
      "outputs": [],
      "source": [
        "nltk.pos_tag(palabras) # La funcion de Pos Tagging de NLTK no incluye el idioma español"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niJD4IqVWF-A"
      },
      "source": [
        "## Dependencias Sintacticas\n",
        "Permite identificar cómo relacionan las palabras teniendo en cuenta las relaciones sintácticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk4Mjg7_WJuB"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(text_sent[0])\n",
        "\n",
        "# For every token in document, print it's tokens to the root\n",
        "for token in doc:\n",
        "    pp.pprint('{} --> {}'.format(token, token.head))\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter = True, options = {'distance': 120})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yvExLqGcctI"
      },
      "source": [
        "## Collocations\n",
        "Son dos o más palabras que tienden a aparecer juntos. Por ejemplo \"Buenos Aires\", \"República Argentina\"\n",
        "\n",
        "Estos pares de palabras se pueden considerar también como un token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_-SsaFmdfyz"
      },
      "outputs": [],
      "source": [
        "! pip install wikipedia\n",
        "import wikipedia\n",
        "subject = ['Maradona', 'Bill Clinton','Barack Obama','Angela Merkel','Argentina']\n",
        "\n",
        "wikipedia.set_lang(\"es\")\n",
        "wikiPage = \",\".join([wikipedia.page(wikipedia.search(w)[0]).content for w in subject])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjOMYo7yJFHt"
      },
      "outputs": [],
      "source": [
        "wikiPage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkWiCjkLbnRd"
      },
      "outputs": [],
      "source": [
        "# Busqueda de la frecuencia de Bigramas\n",
        "from nltk.collocations import BigramCollocationFinder,BigramAssocMeasures\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "words_wiki = word_tokenize(wikiPage)\n",
        "\n",
        "\n",
        "finder = BigramCollocationFinder.from_words(words_wiki)\n",
        "finder.apply_freq_filter(2)\n",
        "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in spanish_stops)\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "mejores_bigramas = list(finder.nbest(bigram_measures.pmi, 100))\n",
        "mejores_bigramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zutJio3D7IGd"
      },
      "outputs": [],
      "source": [
        "resultado = [ bigrama for bigrama in mejores_bigramas if 'argentina' in \" \".join(bigrama).lower()]\n",
        "resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyNZnpn9opLV"
      },
      "outputs": [],
      "source": [
        "# Armado de Trigramas\n",
        "from nltk.collocations import TrigramCollocationFinder,TrigramAssocMeasures\n",
        "\n",
        "finder = TrigramCollocationFinder.from_words(words_wiki)\n",
        "finder.apply_freq_filter(2)\n",
        "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in spanish_stops)\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "mejores_bigramas = list(finder.nbest(bigram_measures.raw_freq, 10))\n",
        "mejores_bigramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALPU-XpqJ83_"
      },
      "source": [
        "## WordNet\n",
        "Base de datos en ingles en la cual se incluyen sinonimos, antonimos, significados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yjSDejnKdgW"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "sinonimos = wordnet.synsets(\"driving\")\n",
        "sinonimos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdMsaxbCLQZ0"
      },
      "outputs": [],
      "source": [
        "# Palabra canonica del primer sinonimo\n",
        "print(sinonimos[0].lemmas()[0].name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOZAY7NiL801"
      },
      "outputs": [],
      "source": [
        "# Definicion\n",
        "print(sinonimos[3].definition())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8lenBEJMH0B"
      },
      "outputs": [],
      "source": [
        "# Ejemplos de uso\n",
        "print(sinonimos[3].examples())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo4It_aiNcvG"
      },
      "outputs": [],
      "source": [
        "print(\"Total:       \" + str(len(nltk.corpus.wordnet.synsets('great'))))\n",
        "print(\"Sustantivos: \" + str(len(nltk.corpus.wordnet.synsets('great', pos='n'))))\n",
        "print(\"Adjetivos:   \" + str(len(nltk.corpus.wordnet.synsets('great', pos='a'))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4GzC7rtN74c"
      },
      "source": [
        "## Corrector ortografico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwAMoJGxN-ek"
      },
      "outputs": [],
      "source": [
        "! apt install -qq enchant\n",
        "! apt install myspell-es\n",
        "! pip install pyenchant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjQvoP5MQjbC"
      },
      "outputs": [],
      "source": [
        "import enchant\n",
        "from nltk.metrics import edit_distance\n",
        "class SpellingReplacer(object):\n",
        "  def __init__(self, dict_name='es', max_dist=2):\n",
        "    self.spell_dict = enchant.Dict(dict_name)\n",
        "    self.max_dist = max_dist\n",
        "  def replace(self, word):\n",
        "    if self.spell_dict.check(word):\n",
        "      return word\n",
        "    suggestions = self.spell_dict.suggest(word)\n",
        "    if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
        "      return suggestions[0]\n",
        "    else:\n",
        "      return word\n",
        "\n",
        "replacer = SpellingReplacer()\n",
        "print(replacer.replace('mineri'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_J2KSyxeTac"
      },
      "source": [
        "## Vectorizacion de Tokens\n",
        "El texto que estamos analizando es no estructurado, las técnicas de vectorización de tokens nos permiten generar un espacio de características para que pueda ser utilizado en los modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTAwP5TegOsd"
      },
      "source": [
        "### Bag of word - BOW\n",
        "Cada documento corresponde a una fila, cada token a una columna y como valor se asigna la cantidad de veces que esta ese token en el documento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QZtcT10fE4y"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "data=[\n",
        "      \"the Ramiess sings classic songs\",\n",
        "      \"the he he listens to old pop \",\n",
        "      \"the the and rock music\",\n",
        "      \"the and also listens to classical songs\",\n",
        "      \"the listens songs\"]\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "data_tranform = count_vectorizer.fit_transform(data)\n",
        "\n",
        "pd.DataFrame.sparse.from_spmatrix(data_tranform, columns=count_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R--mK2frt_1S"
      },
      "outputs": [],
      "source": [
        "data_tranform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0uKlRr6iVbO"
      },
      "outputs": [],
      "source": [
        "# Vocabulario -> El objetivo de relizar los procesos de lematizacion, stemmer y stopwords es reducir el vocabulario posible\n",
        "print(count_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uJrUcpN0UCX"
      },
      "source": [
        "Consideraciones a tener en cuenta:\n",
        "* No respeta el orden de las palabras: Si tengo dos frases diferentes pero con las mismas palabras no las voy a poder identificar -> \"no, tengo dinero\" \"no tengo dinero\"\n",
        "* Generalmente se tiene una dimensionalidad alta, se tendrá una característica por cada palabra en el corpus\n",
        "* Como resultado se tiene una matriz dispersa, es decir la gran mayoría de los valores corresponden al 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5e-vIY-gTAy"
      },
      "source": [
        "### TF-IDF\n",
        "En corpus de texto extensos, algunas palabras estarán muy presentes (por ejemplo, \"the\", \"a\", \"is\" en inglés), por lo tanto, llevarán muy poca información significativa sobre el contenido real del documento.\n",
        "Si tuviéramos un conteo directo, esos términos se posicionarán sobre los términos más interesantes.\n",
        "\n",
        "Una forma de ponderar los tokens teniendo en cuenta las frecuencias de las palabras es utilizar tf-idf\n",
        "\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "Supongamos tenemos un corpus de **10 M** de documentos donde la palabra **feliz** aparece **1.000** veces dentro del corpus.\n",
        "\n",
        "Al mismo tiempo un documento específico de **100** palabras tiene la  palabra ***feliz*** aparece 5 veces.\n",
        "* **TF**= (5/100) = **0.05**\n",
        "* **IDF**= log(10 M / 1,000) = **4**\n",
        "* **TF-IDF** =  0.05 × 4 = **0.20**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B5gXXHygVOI"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "data_tranform = tfidf_vectorizer.fit_transform(data)\n",
        "\n",
        "pd.DataFrame.sparse.from_spmatrix(data_tranform, columns=tfidf_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g-rZl7-vrXh"
      },
      "outputs": [],
      "source": [
        "data_tranform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fohLMGHD5hoC"
      },
      "source": [
        "El objetivo de usar tf-idf en lugar de las frecuencias de ocurrencias es reducir el impacto de los tokens más frecuentes, por lo tanto, son empíricamente menos informativos que las características que ocurren en un determinado corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QMJyZT14u7F"
      },
      "source": [
        "## Práctica\n",
        "Definir una función de python que reciba como parámetro un texto y realice la limpieza y depuración de datos teniendo en cuenta las funcionalidades vistas en esta práctica.\n",
        "\n",
        "Pasos recomensados:\n",
        "* Tokenizacion\n",
        "* Stopwords\n",
        "* Corrector ortografico\n",
        "* Lematizacion o Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTxpPisFJAaW"
      },
      "outputs": [],
      "source": [
        "! pip install -q wikipedia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUfdaWEX6skn"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "\n",
        "subject = ['Barack Obama']\n",
        "\n",
        "wikipedia.set_lang(\"es\")\n",
        "wikiPage = wikipedia.page(wikipedia.search(subject)[0]).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9aq9POY_dOL"
      },
      "outputs": [],
      "source": [
        "wikiPage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrNrvBby4wxs"
      },
      "outputs": [],
      "source": [
        "def pre_procesamiento_texto(text):\n",
        "   #tokenizador\n",
        "   #stemer\n",
        "   #....\n",
        "  return text.split()\n",
        "\n",
        "results = pre_procesamiento_texto(wikiPage)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROipvSNJ7HEr"
      },
      "outputs": [],
      "source": [
        "from nltk.text import Text\n",
        "t = Text(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77y36NJkIjUh"
      },
      "outputs": [],
      "source": [
        "t.plot(20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8tIzFCg0TuTw",
        "rK1Re_rxZXqF",
        "-g34N2n7t4A-",
        "DyXNgypVu9p5",
        "2AKDhvCmzLr_",
        "niJD4IqVWF-A",
        "3yvExLqGcctI",
        "ALPU-XpqJ83_",
        "L4GzC7rtN74c",
        "m5e-vIY-gTAy"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}